---
layout: post
title: "kddcup2015 note"
description: ""
category: 
tags: []
published: false
---


1.  Data Leakage

	Introducing features basicaly only powerful in training set, therefore reducing the discriminativeness of the model

2.  Class imbalance problem

	Roughly drop: not-drop is 4: 1. If we balance the class by resampling from the not-drop pool, it doesn't make much difference. 

3.  Whitening seems to be beneficial

4.  Doing Ensembling??

5.  AdaBoost is pretty robust. 

6.  Gradient Boosting Tree is even better

7.  Model fusion is not effective. Maybe the way to fuse matters. Especially, late fusion will significantly overfit. 